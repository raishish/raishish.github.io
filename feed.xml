<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://raishish.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://raishish.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-18T17:23:41+00:00</updated><id>https://raishish.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">How is bidirectional information retrieved and generated in masked diffusion language models?</title><link href="https://raishish.github.io/blog/2025/discrete-diffusion-rome/" rel="alternate" type="text/html" title="How is bidirectional information retrieved and generated in masked diffusion language models?"/><published>2025-07-14T00:00:00+00:00</published><updated>2025-07-14T00:00:00+00:00</updated><id>https://raishish.github.io/blog/2025/discrete-diffusion-rome</id><content type="html" xml:base="https://raishish.github.io/blog/2025/discrete-diffusion-rome/"><![CDATA[<h2 id="summary">Summary</h2> <h3 id="tldr">tl;dr</h3> <p>In this post, I will discuss the intermediate progress on using ROME to study bidirectional information retrieval in masked (discrete) diffusion language models.</p> <p>Analysis in-progress!</p> <h2 id="abstract">Abstract</h2> <p>This research proposal aims to conduct the first mechanistic investigation of bidirectional retrieval and generation in masked diffusion language models. We focus on masked diffusion models (MDMs), which have shown surprising resistance to the “reversal curse” that plagues autoregressive architectures. The study will map the causal pathways of knowledge recall and analyze the internal representations learned by the model. By leveraging recent insights into the “factorization curse” - a more fundamental problem underlying the reversal curse - we seek to understand how non-autoregressive architectures overcome these limitations. This mechanistic investigation will be among the first to systematically examine the internal workings of masked diffusion language models, providing insights into their emerging capabilities for bidirectional logical reasoning. The findings will contribute to the development of more robust language models with improved reasoning abilities and more reliable knowledge representation.</p> <h2 id="introduction">Introduction</h2> <p>Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but recent studies have identified a significant limitation called the “reversal curse.” When autoregressive models like GPT learn a fact such as “A is B,” they fail to automatically deduce the reverse relationship “B is A” <d-cite key="berglund2023reversal"></d-cite>. For example, if a model learns that “Paris is the capital of France,” it struggles to answer “The capital of France is <em>__</em>” correctly.</p> <p>Recent work by <d-cite key="kitouni2024factorization"></d-cite> reframes this as the “factorization curse” - a more fundamental problem where language models fail to learn the same joint distribution under different factorizations. This insight suggests the issue isn’t just about reversing relationships, but about a broader limitation in how next-token prediction objectives force models to learn specific factorizations of joint distributions, undermining their ability to reason bidirectionally.</p> <p>Certain model architectures — particularly bidirectional encoder models like BERT and masked diffusion models (MDMs) — appear to be more resistant to this limitation (<d-cite key="wu2023exploring, nie2024scaling"></d-cite>). Understanding why these architectural differences lead to better bidirectional reasoning capabilities could provide valuable insights for developing more robust AI systems that overcome the factorization curse. This research proposal aims to investigate the mechanisms behind bidirectional information retrieval in language models, with a specific focus on masked diffusion models (MDMs).</p> <h2 id="background-and-related-work">Background and Related Work</h2> <p>The reversal curse was first formally identified in <d-cite key="berglund2023reversal"></d-cite>, which demonstrated that autoregressive models fine-tuned on statements like “A is B” failed to generalize to “B is A.” The authors showed that models like GPT-3 (175B) and Llama-1 (7B) scored no better than random chance when evaluated on reversed relationships.</p> <figure id="reversal-curse"> <img src="/assets/img/2025-discrete-diffusion-rome/reversal-curse.png" alt="The Reversal Curse"/> <figcaption><strong>Figure 1:</strong> The Reversal Curse</figcaption> </figure> <p>Kitouni et al. (2024) significantly expanded the understanding of this phenomenon by reframing it as the “factorization curse” - a fundamental limitation of the next-token prediction objective used in most LLMs <d-cite key="kitouni2024factorization"></d-cite>. Through controlled experiments using their novel WikiReversal benchmark based on Wikipedia knowledge graphs, they demonstrated that this is an inherent failure mode that cannot be solved merely through scaling, reversed tokens, or even naive bidirectional-attention training. Their work identified factorization-agnostic objectives as a promising solution, showing significant improvements across various tasks of different complexity levels.</p> <p>Ma et al. (2023) investigated this problem specifically in the context of model editing, introducing the “Bidirectional Assessment for Knowledge Editing” (BAKE) benchmark and the “Bidirectionally Inversible Relationship moDeling” (BIRD) method <d-cite key="ma2023untying"></d-cite>. Their work revealed that while existing editing methods can effectively recall facts in the direction they were trained on, they perform poorly in the reverse direction. For instance, LLaMA-2 edited with state-of-the-art ROME could recall 99.70% of editing facts in the forward direction but only 0.26% in the reverse direction.</p> <p>Wu and Wang (2023) explored this phenomenon further, comparing autoregressive decoder-only models (like GPT) with bidirectional encoder models (like BERT) <d-cite key="wu2023exploring"></d-cite>. They found that BERT-style models were largely immune to the reversal curse for basic logical deductions, suggesting that architectural differences play a critical role.</p> <p>Recent advances in masked diffusion models have shown particularly promising results. Nie et al. (2024) demonstrated that a 1.1B parameter MDM could break the reversal curse that much larger autoregressive models struggle with <d-cite key="nie2024scaling"></d-cite>. Similarly, research on LLaDA (Large Language Diffusion Models) has shown that diffusion-based approaches effectively address bidirectional reasoning challenges and even outperform GPT-4o on certain reversal tasks <d-cite key="nie2025largelanguagediffusionmodels"></d-cite>.</p> <details> <summary><strong>A primer on masked discrete diffusion</strong></summary> </details> <h2 id="research-objectives">Research Objectives</h2> <p>This study aims to:</p> <ol> <li><strong>Map the causal pathways of knowledge recall</strong> to understand how information flows when retrieving facts in forward versus reverse directions.</li> <li><strong>Analyze learned representations</strong> to determine if and how they encode bidirectional logical relationships, identifying any symmetries in forward and backward representation of concepts.</li> </ol> <h2 id="methodology">Methodology</h2> <h3 id="models">Models</h3> <ul> <li>Masked Diffusion Language Model: <strong>SMDM-1028M</strong> - Llama2 based 1B masked diffusion model from <d-cite key="nie2024scaling"></d-cite></li> <li>(Baseline) Autoregressive Models: <strong>AR-1028M</strong> - Llama2 based 1B autoregressive decoder only model from <d-cite key="nie2024scaling"></d-cite></li> </ul> <table id="tab-model-config"> <caption><strong>Table 1:</strong> Model configuration parameters used in experiments.</caption> <thead> <tr> <th>Model Type</th> <th>Masked Diffusion</th> <th>Autoregressive</th> </tr> </thead> <tbody> <tr> <td>Base Architecture</td> <td>Llama2</td> <td>Llama2</td> </tr> <tr> <td>Transformer Blocks</td> <td>20</td> <td>20</td> </tr> <tr> <td>Attention Heads</td> <td>14</td> <td>14</td> </tr> <tr> <td><code>n_embed</code></td> <td>1792</td> <td>1792</td> </tr> <tr> <td><code>embed_dim</code></td> <td>7168</td> <td>7168</td> </tr> <tr> <td><code>vocab_size</code></td> <td>32000</td> <td>32000</td> </tr> <tr> <td>Sampling temperature</td> <td>0.</td> <td>0.</td> </tr> <tr> <td>Sampling algorithm</td> <td>Greedy</td> <td>Greedy</td> </tr> </tbody> </table> <table id="smdm-sampling-config"> <caption><strong>Table 2:</strong> Sampling configuration for SMDM-1028M (Masked Diffusion)</caption> <thead> <tr> <th>Config</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Denoising Steps</td> <td>16</td> </tr> <tr> <td>Context Length</td> <td>52</td> </tr> <tr> <td>CFG Scale</td> <td>0.8</td> </tr> </tbody> </table> <p>The models chosen in this study are pre-trained on the SlimPajama Dataset <d-cite key="nie2024scaling"></d-cite>. These models are based on the Llama2 architecture and are configured with settings referenced in <a href="#tab-model-config">Table 1</a>. The sampling config for the masked diffusion language model SMDM-1028M has been borrowed from <d-cite key="nie2024scaling"></d-cite> as well and is shown in <a href="#smdm-sampling-config">Table 2</a>. The denoising steps indicate the number of steps over which the masked model response is unmasked. The context length refers to the total length of the sequence given as input to model, which includes both the prompt and the unmasked reponse <a href="#masked-diffusion-inference">Fig 2</a>. The CFG scale is the factor with which the classifier free guidance is scaled and applied for conditional generation.</p> <figure id="masked-diffusion-inference"> <img src="/assets/img/2025-discrete-diffusion-rome/masked_response.png" alt="Denoising response sequence during inference in masked diffusion language models"/> <figcaption><strong>Figure 2:</strong> The prompt token is padded with masked tokens which are iteratively denoised during sampling from a masked diffusion language model.</figcaption> </figure> <h3 id="dataset">Dataset</h3> <p>The reversal curse can be studied with factual recall on:</p> <ul> <li>Completions based on pre-trained data</li> <li>Completions based on SFT-trained data</li> </ul> <p>The masked diffusion model was first prompted to generate single-token completions on Wikipedia knowledge prompts, which the model is trained on as part of SlimPajama. The last token did not elicit the most informative tokens. A subset of the responses can be seen in <a href="#wikipedia-knowledge-prompt-completions">Fig 3</a>.</p> <figure id="wikipedia-knowledge-prompt-completions"> <img src="/assets/img/2025-discrete-diffusion-rome/pre-training-single-token-completions.png" alt="Wikipedia Knowledge Prompt Completions"/> <figcaption><strong>Figure 3:</strong> Generations by SMDM-1028M on Wikipedia knowledge prompts yields high entropy (less information in first generated token) and could not be used as a controlled database to test the reversal curse on.</figcaption> </figure> <p>The models were then fine tuned on the Reversal curse dataset and attained high accuracy <a href="#tab-reversal-curse-eval">Table 3</a>, so we chose this dataset. The samples from the dataset are depicted in <a href="#dataset">Fig. 4</a>.</p> <figure id="dataset"> <img src="/assets/img/2025-discrete-diffusion-rome/dataset.png" alt="Dataset"/> <figcaption><strong>Figure 4:</strong> The reversal curse dataset.</figcaption> </figure> <h3 id="causal-tracing">Causal Tracing</h3> <p>\cite{Meng2022LocatingAE} introduced causal tracing as a method to identify storage and processing of factual knowledge in language models. The technique aims to isolate the causal effect of individual states within the network while processing factual statements, essentially mapping the information flow path through the model.</p> <p>The method aims to understand the model’s prediction, focusing on identifying key neurons and layers involved in recalling factual associations.</p> <h4 id="tracing-information-flow">Tracing Information Flow</h4> <d-cite key="Meng2022LocatingAE"></d-cite> <p>use causal mediation analysis to quantify the contribution of intermediate variables (hidden states) in the model’s causal graph to a correct factual prediction <d-cite key="10.5555/2074022.2074073, vig2020investigating"></d-cite>.</p> <p>To calculate each state’s contribution, they observe the model’s internal activations during three runs:</p> <ol> <li><strong>Clean Run:</strong> A factual prompt $x$ is passed into the model $G$, and all hidden activations are collected.</li> <li><strong>Corrupted Run:</strong> The subject $s$ is obfuscated from $G$ before the network runs to damage the prediction.</li> <li><strong>Corrupted-with-Restoration Run:</strong> The model runs computations on the noisy embeddings as in the corrupted baseline, except at some token $i$ and layer $l$, the model is forced to output the clean state $h^{(l)}_i$.</li> </ol> <p><strong>Indirect Effect (IE):</strong> The indirect effect of a specific mediating state $h^{(l)}_i$ is defined as the difference between the probability of $o$ under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted.</p> <p><strong>Average Indirect Effect (AIE):</strong> Averaging over a sample of statements, the average total effect (ATE) and average indirect effect (AIE) is obtained for each hidden state variable.</p> <h4 id="extracting-factual-representations">Extracting Factual Representations</h4> <p>Meng et al., 2022 <d-cite key="Meng2022LocatingAE"></d-cite> build upon the localized factual association hypothesis to devise a method to <strong>localize factual associations</strong> within transformer models. By locating the layer and weight matrix where a new fact can be injected with minimal interference, ROME provides evidence that factual knowledge is encoded in specific MLP projection layers as <strong>linear associations</strong> between key and value vectors. The proposed method by <d-cite key="Meng2022LocatingAE"></d-cite> is explained briefly for context.</p> <p><strong>Extracting the Key $k_{\ast}$</strong></p> <p>To identify the memory location corresponding to the subject $s$ in the factual triple $(s, r, o)$ a key vector $k_{\ast}$ is constructed with the underlying assumption being certain activations within the MLP are responsible for retrieving facts associated with a given subject. To find a robust key, the activation vectors are averaged across multiple short text prefixes that lead into and end with the subject token $s$.</p> \[k_{\ast} = \frac{1}{N} \sum_{j=1}^N k(x_j + s)\] <p>where $k(x)$ is derived from the layer $l^*$’s MLP activations after the non-linearity is applied.</p> <p><strong>Extracting the Value $v_{\ast}$</strong></p> <p>Meng et al., 2022 <d-cite key="Meng2022LocatingAE"></d-cite> compute a value vector $v_{\ast}$ that encodes a new object $o^*$ (i.e., the fact to be recalled) as a property of the subject $s$. This is formulated as an optimization problem with a linear combination of two objectives:</p> <ol> <li>Maximize the likelihood that the model, when prompted with subject $s$ and relation $r$, predicts the new object $o^*$.</li> <li>Minimize the KL divergence between the model’s original predictions for $s$ and its predictions after editing, to prevent disrupting the subject’s broader semantic identity.</li> </ol> <p>This results in the following loss objective:</p> \[\mathcal{L}(z) = - \frac{1}{N} \sum_{j=1}^N \log \mathbb{P}_{G(m_i^{(l^*)} := z)}(o^* \mid x_j + p) + D_{\mathrm{KL}}\left( \mathbb{P}_{G(m_{i'}^{(l^*)} := z)}[x \mid p'] \,\Vert\, \mathbb{P}_G[x \mid p'] \right)\] <p>where $z$ is optimized to serve as $v_{\ast}$, the value that, when output by the MLP, causes the model to recall the new object $o^*$ in response to the original factual prompt $p$. $p’$ represents the set of adjacent prompts (of the form “{subject} is a”) which aim to preserve the model’s understanding of the subject’s essence.</p> <p>In contrast, <strong>we aim to determine the representation of the original object $o$ when preceded with subject $s$ in the prompt</strong>. So we update the objective function to account for $o$ instead of $o^*$ and the rest remains the same:</p> \[\mathcal{L}(z) = - \frac{1}{N} \sum_{j=1}^N \log \mathbb{P}_{G(m_i^{(l^*)} := z)}(o \mid x_j + p) + D_{\mathrm{KL}}\left( \mathbb{P}_{G(m_{i'}^{(l^*)} := z)}[x \mid p'] \,\Vert\, \mathbb{P}_G[x \mid p'] \right)\] <p>Meng et al., 2022 <d-cite key="Meng2022LocatingAE"></d-cite> go a step further and apply a <strong>rank-one update</strong> to the projection matrix $W^{(l)}_{\text{proj}}$ of the MLP at layer $l^*$, in order to encode a new key–value pair into the model’s weights. This is beyond the scope of this research study.</p> <h4 id="causal-tracing-for-diffusion-language-models">Causal Tracing for Diffusion Language Models</h4> <p>Unlike autoregressive and masked language models, which predict all response tokens in a single forward pass; response generation in masked diffusion language models starts with a sequence of all masked tokens $x_1$ with fixed length $L$ (context length) is set and a mask predictor $f_\theta$ unmasks all tokens following a noise schedule $\alpha_t$ over $T$ denoising steps to generate the response $x_0$. Specifically, the masked diffusion model simulates a diffusion process from $t = 1$ (fully masked) to $t = 0$ (unmasked), predicting all masks simultaneously at each step with the possibility of flexible remasking strategies (<a href="#unmasking-during-inference">Fig. 5</a>).</p> <figure id="unmasking-during-inference"> <img src="/assets/img/2025-discrete-diffusion-rome/unmasking-during-inference.png" alt="Unmasking tokens via sampling during inference"/> <figcaption><strong>Figure 5:</strong> RMasked diffusion models simulate a diffusion process from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with the possibility of flexible remasking strategies.</figcaption> </figure> <p>For masked diffusion language modeling, the individual response tokens (as seen in <a href="#masked-diffusion-generation">Fig. 6</a>) can be unmasked at different time steps.</p> <figure id="masked-diffusion-generation"> <img src="/assets/img/2025-discrete-diffusion-rome/masked-diffusion-generation.png" alt="Response generation from masked diffusion language models"/> <figcaption><strong>Figure 6:</strong> Response generation from masked diffusion language model.</figcaption> </figure> <p>During training, Masked diffusion models (MDMs) are trained to increased the likelihood of the data distribution for different masking conditions. During inference, the tokens can be masked with different sampling strategies to determine the order in which the response tokens are predicted. In the context of causal information flow analysis, this introduces both challenges and opportunities:</p> <ul> <li><strong>Challenge:</strong> This introduces multiple causal paths from a subject token across different denoising time steps.</li> <li><strong>Opportunity:</strong> By quantifying the information flow in the first denoising time step, often the most important one, we can isolate the information flow from the subject tokens and the intermediate unmasked object tokens (which the model can attend to from the second time step).</li> </ul> <p>Causal tracing is run from time steps $t = t_1$ to $t = t_2$.</p> <p>When retrieving information from a language model, the process acts like a key-based value lookup with the subject $s$ acting as a <code class="language-plaintext highlighter-rouge">key</code> and the object acting as a <code class="language-plaintext highlighter-rouge">value</code> to be extracted. Following <d-cite key="Meng2022LocatingAE"></d-cite>, the key vectors in MLP layers corresponding to the last subject token in identified important MLP units (discovered by causal tracing) will be fixed and the objective function defined in equation 2 will be optimized to extract the representation of the object $o$.</p> <p>This procedure will be done while trying to elicit knowledge in both forward (sample prompt: “The trailblazer known as Daphne Barrington was once”) and reverse (sample prompt: “Immersed in the world of directing the virtual reality masterpiece, A Journey Through Time.”) directions. The key and value representations in these cases would be interchanged between both runs. For example, the key in prompt 1 “Daphne Barrington” would act as the value in prompt 2. The aim is to compare the key and value representations of the same concept via dimensionality reduction techniques (SVD and PCA) to see if there exists a symmetry in which the information is encoded in the diffusion model.</p> <h2 id="current-progress">Current Progress</h2> <p>We begin with a scaling experiment to study the impact of sampling steps on knowledge recall, measured with exact match accuracy <d-cite key="berglund2023reversal"></d-cite>. As the sampling steps are increased, there is an increase in knowledge recall in both forward and reverse directions (refer <a href="#tab-reversal-curse-eval">Table 3</a>).</p> <div class="alert alert-warning"> Please note that these accuracy values for <code>Description2Person</code> reverse and <code>Person2Description</code> reverse are different from the ones reported in <d-cite key="nie2024scaling"></d-cite>. *We believe this is an error in reporting of the values since reverse recall is harder to model for language models, majorly because the perplexity of the text in description is usually higher than the text in person names of the Reversal Curse dataset <d-cite key="berglund2023reversal"></d-cite>.* </div> <table id="tab-reversal-curse-eval"> <caption><strong>Table 3:</strong> Scaling sampling steps leads to better knowledge recall performance in forward and reverse directions.</caption> <thead> <tr> <th rowspan="2">Sampling Steps</th> <th colspan="2">Description2Person</th> <th colspan="2">Person2Description</th> </tr> <tr> <th>Same (easy)</th> <th>Reverse (difficult)</th> <th>Same (difficult)</th> <th>Reverse (easy)</th> </tr> <tr> <th></th> <th>% Acc ↑</th> <th>% Acc ↑</th> <th>% Acc ↑</th> <th>% Acc ↑</th> </tr> </thead> <tbody> <tr> <td>1 step</td> <td>91.3</td> <td>0</td> <td>0</td> <td>40.3</td> </tr> <tr> <td>2 steps</td> <td>97</td> <td>0.7</td> <td>2.3</td> <td>79</td> </tr> <tr> <td>4 steps</td> <td>96.3</td> <td>17</td> <td>25</td> <td>87.7</td> </tr> <tr> <td>8 steps</td> <td>97.3</td> <td>27.3</td> <td>37.7</td> <td>87.3</td> </tr> <tr> <td>16 steps</td> <td><strong>98</strong></td> <td><strong>31</strong></td> <td>42</td> <td>88.3</td> </tr> <tr> <td>32 steps</td> <td>97.7</td> <td>29.7</td> <td><strong>43</strong></td> <td><strong>90</strong></td> </tr> </tbody> </table> <p>This is a work-in-progress. More results coming soon!</p> <p>The existing code for the study can be found at <a href="https://github.com/raishish/diffusion-interp">raishish/diffusion-interp</a>.</p> <h2 id="future-work">Future Work</h2> <h2 id="acknowledgements">Acknowledgements</h2> <h2 id="references">References</h2> <p>If you would like to cite this work, please use the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rai2025discrete-diffusion-rome</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{How is information retrieved and generated in masked diffusion language models?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Rai, Ashish}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{July}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://raishish.github.io/blog/2025/discrete-diffusion-rome/}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Ashish Rai</name></author><category term="masked-discrete-diffusion"/><category term="mech-interp"/><category term="rome"/><summary type="html"><![CDATA[Understanding Bidirectional Information Retrieval in MDLMs with ROME]]></summary></entry><entry><title type="html">Characterizing arithmetic length generalization performance in large language models</title><link href="https://raishish.github.io/blog/2025/characterizing-arithmetic-length-generalization/" rel="alternate" type="text/html" title="Characterizing arithmetic length generalization performance in large language models"/><published>2025-02-03T00:00:00+00:00</published><updated>2025-02-03T00:00:00+00:00</updated><id>https://raishish.github.io/blog/2025/characterizing-arithmetic-length-generalization</id><content type="html" xml:base="https://raishish.github.io/blog/2025/characterizing-arithmetic-length-generalization/"><![CDATA[<h2 id="summary">Summary</h2> <p><strong>tl;dr</strong></p> <p>We tested arithmetic capabilities of GPT-4o-mini, LLaMa-3.1-70B, LLaMa-3.1-8B, and Gemma-2b-it on increasingly complex numbers. While these models achieve very well (~94%) on GSM-8K, their performance degrades significantly with number complexity: addition works up to 6-digit numbers then fails, multiplication drops to 0% accuracy after 3-digit numbers, unique digit count matters more than number size (e.g., “48346” is harder than “480000”). Models consistently get first/last digits right even when overall answers are wrong, and mechanistic analysis shows final numerical answers emerge in later transformer layers.</p> <p><strong>Bottom line:</strong> Current LLMs have clear arithmetic boundaries that aren’t solved by scale alone.</p> <h3 id="team">Team</h3> <p>Team members: Ashish Rai, <a href="https://www.linkedin.com/in/akashpeddaputha/">Akash Peddaputha</a>, <a href="https://www.linkedin.com/in/the-aman-gupta/">Aman Gupta</a></p> <p>Advised by: <a href="https://hhexiy.github.io/">Professor He He</a> (as part of the NLP graduate level course at NYU, <a href="https://nyu-cs2590.github.io/fall2024/">Fall 2024</a>)</p> <p>Full report (with additional results): <a href="https://github.com/raishish/arithmetic-interp/blob/main/length-generalization-of-arithmetic-performance.pdf">link</a>.</p> <h3 id="research-approach">Research Approach</h3> <p>We designed a two-pronged approach to investigate this phenomenon:</p> <ol> <li> <p><strong>Standard Benchmark Testing</strong>: We first evaluated model performance by perturbing numerical values for questions in the GSM-8K dataset, a standard mathematical reasoning benchmark containing grade-school-level problems.</p> </li> <li> <p><strong>Custom Dataset Creation</strong>: We then developed our own Arithmetic Length Generalization Dataset (ALGD) with 3,500 examples specifically designed to test scaling behavior across different operations:</p> <ul> <li>Addition</li> <li>Multiplication</li> <li>Subtraction</li> <li>Division</li> <li>Modulus</li> </ul> </li> </ol> <p>For each operation, we created 100 examples with progressively more complex numbers, ranging from 1-digit to 7-digit values, allowing us to precisely identify where performance drops.</p> <h3 id="key-discoveries">Key Discoveries</h3> <h4 id="baseline-performance">Baseline Performance</h4> <p>Our initial evaluation confirmed that frontier models perform impressively on standard mathematical reasoning tasks. Both GPT-4o-mini and LLaMa-3.1-70B-Instruct achieved approximately 94% accuracy on the unmodified GSM-8K dataset. This established a strong baseline for our subsequent experiments.</p> <h4 id="length-generalization-patterns">Length Generalization Patterns</h4> <p>When we increased numerical complexity, we observed fascinating patterns:</p> <ol> <li> <p><strong>Simple Scaling Works Well</strong>: When we multiplied values in GSM-8K problems by constants (like 1000), model performance remained strong. For example, models handled “48” and “480000” with similar accuracy.</p> </li> <li> <p><strong>Unique Digits Matter More Than Size</strong>: The critical factor affecting performance wasn’t merely the size of numbers but their complexity in terms of unique digits. Models struggled significantly with smaller numbers containing more diverse digits (like “48346”) compared to larger but simpler numbers.</p> </li> <li><strong>Operation-Specific Thresholds</strong>: We identified clear performance thresholds for different operations: <ul> <li><strong>Addition</strong>: Models generally maintained accuracy up to 6-digit numbers</li> <li><strong>Multiplication</strong>: Performance declined sharply after 3-digit numbers, reaching 0% for larger values</li> </ul> </li> <li><strong>Edge Digits vs. Middle Digits</strong>: One particularly interesting finding was that models tended to get the first and last few digits correct even when overall accuracy declined. The middle digits showed consistently higher error rates.</li> </ol> <h4 id="mechanistic-insights">Mechanistic Insights</h4> <p>Using interpretability techniques like Tuned Lens (built upon Logit Lens), we examined how responses are generated at different layers of the network. Our analysis revealed:</p> <ol> <li>The final numerical answers are primarily generated in the later layers of the model</li> <li>Even when providing the first token of multi-token answers in the prompt, models frequently failed to generate subsequent tokens correctly.</li> </ol> <h3 id="implications-and-future-directions">Implications and Future Directions</h3> <p>Our findings highlight important limitations in current LLMs’ arithmetic capabilities. While these models excel at numerous language tasks and can handle basic calculations, they demonstrate clear boundaries in their ability to process complex numerical operations.</p> <p>As researchers, we’re now focused on:</p> <ol> <li><strong>Mechanistic Understanding</strong>: Using white-box experiments to analyze why these limitations occur</li> <li><strong>Intermediate Decoding</strong>: Examining model outputs at different stages to identify where errors emerge</li> <li><strong>Component Analysis</strong>: Identifying specific architectural elements that influence arithmetic performance</li> </ol> <p>Our goal isn’t necessarily to improve arithmetic operations in LLMs (specialized tools already exist for this purpose) but rather to understand how numerical complexity affects model behavior and reasoning.</p> <h2 id="abstract">Abstract</h2> <p>Large language models (LLMs) excel at single-pass NLP tasks like text generation, but struggle with unbounded multistep computations like arithmetic operations with large numbers. This study aims to assess and formulate the length generalization of arithmetic reasoning of LLMs. We observed a significant degradation in model performance when the questions were rephrased with the numerical values scaled in length when tested on the GSM-8K benchmark. We further investigated the scaling behavior on arithmetic tasks and found that state-of-the-art models like GPT-4o-mini and LLaMa-3.1-70B can generate accurate outputs for 4-digit addition and 3-digit multiplication. However, accuracy declines sharply with larger numbers, particularly when the number of unique digits increases. Our results show that while models can generally handle the addition of numbers containing 6 digits and multiplication of 3-digit numbers, they often fail for higher orders. Despite errors on higher-order numbers, we observe a pattern in digit-wise accuracy: the first and the last few digits have higher accuracy than those in the middle, highlighting specific numerical limits in LLM’s capabilities for arithmetic tasks. The dataset is uploaded to <a href="https://huggingface.co/datasets/raishish/ALGD">Hugging Face</a> and the code for reproduction are publicly available at <a href="https://github.com/raishish/arithmetic-interp">https://github.com/raishish/arithmetic-interp</a>.</p> <h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) have shown remarkable prowess in various domains, such as natural language processing, question answering, and creative tasks <d-cite key="gunter2024appleintelligencefoundationlanguage,Mirzadeh2024,openai2024gpt4technicalreport"></d-cite>. Nevertheless, they still face challenges when it comes to arithmetic operations. Solving math problems necessitates a combination of skills, including text comprehension and arithmetic calculation <d-cite key="opedal2024languagemodelsexhibitcognitive"></d-cite>. Research indicates that the reasoning process employed by LLMs is often based on probabilistic pattern-matching rather than formal logical reasoning <d-cite key="Mirzadeh2024,jiang2024peektokenbiaslarge"></d-cite>.</p> <p>Recent studies on Transformers indicate that models specifically trained on addition and other arithmetic tasks encounter difficulties in generalizing to varying lengths and complexities of numbers <d-cite key="lee2023teachingarithmeticsmalltransformers"></d-cite>. Even when fine-tuned from pre-trained LLMs, these models continue to struggle with fundamental algorithmic tasks, highlighting the disparity in their capacity to effectively handle numeric complexity <d-cite key="dziri2023faithfatelimitstransformers"></d-cite>.</p> <p>LLMs excel in text generation and summarization, but upon closer examination their limitations in mathematical reasoning capabilities become apparent. In this project, we systematically investigate the arithmetic reasoning of LLMs by examining their generalization performance on elementary arithmetic tasks. We modify numerical values in the GSM-8K benchmark <d-cite key="cobbe2021training"></d-cite> to assess the sensitivity of models like GPT-4o-mini and LLaMa-3.1-70b-Instruct to increasing numerical complexity. Additionally, we explore the scaling behavior of LLMs on these tasks by gradually increasing the complexity of numerical values, testing their accuracy on elementary arithmeric operations from 1-digit to 7-digit numbers. Our experiments show that LLMs can handle simpler arithmetic tasks but their accuracy deteriorates with increasing complexity, especially for smaller numbers with higher unique digit counts.</p> <p>Through this study, we aim to understand the impact of numerical complexity and arithmetic operations on the scaling behavior of LLMs, rather than improving arithmetic operations.</p> <h2 id="related-work">Related Work</h2> <p>Recent advances in LLMs have spurred interest in improving their mathematical reasoning capabilities. <d-cite key="Mirzadeh2024"></d-cite> introduces GSM-Symbolic, an improved benchmark to evaluate LLMs on math reasoning tasks. Their approach reveals the fragility of LLM reasoning, with model performance sensitive to slight numerical changes. <d-cite key="Zhang2024"></d-cite> further explores LLM reasoning mechanisms by identifying critical attention heads and MLPs essential for arithmetic calculations.</p> <p>The work in <d-cite key="Zhou2024"></d-cite> addresses scaling behavior of LLMs across numeral systems, comparing base-10 and base-100 tokenization schemes. <d-cite key="Ahn2024"></d-cite> provides a survey on LLMs in mathematical reasoning, categorizing problem types and identifying relevant datasets for arithmetic, geometry, and theorem proving. Finally, the work by <d-cite key="Zhou2024_Algorithms"></d-cite> investigates transformers’ length generalization capabilities. It focuses on transformer architectural aspects that affect their generalization on length-variable tasks.</p> <p>Our approach builds on these works by analyzing the effects of symbolic reasoning templates and tokenization schemes on mathematical problem-solving accuracy, providing an extended evaluation across larger datasets.</p> <h2 id="approach">Approach</h2> <p>We evaluated the model’s performance on the GSM-8K dataset by multiplying numerical values by multiples of 10, which did not increase the complexity much since the number of unique digits remains constant. The model consistently produced correct answers for such questions. To further increase complexity, we introduced random numbers with more unique digits and tested the model’s handling of these variations. We also used one-shot and few-shot and Chain of Thought (CoT) prompting to improve multi-step reasoning. The models were prompted to not use or generate code to generate responses, thus avoiding the impact of code interpreting techniques. We experimented with different sampling settings and conducted repeated tests under similar conditions to ensure consistency.</p> <p>To mechanistically understand how responses are generated for arithmetic operations, we use transformer interpretability techniques to look for attribution of intermediate layers of the network to the final response.</p> <h2 id="experiments">Experiments</h2> <h3 id="data">Data</h3> <p>In this work, we evaluate our models using two datasets: the widely-used GSM-8K test dataset and a custom dataset we created for evaluating length generalization of arithmetic performance. For both datasets, the task involves generating an accurate numerical output in response to the arithmetic input or question.</p> <h4 id="gsm-8k-dataset">GSM-8K Dataset</h4> <p>For evaluating our models’ mathematical reasoning capabilities, we use the GSM-8K dataset <d-cite key="cobbe2021training"></d-cite>. GSM-8K contains grade-school-level math problems designed to assess logical reasoning and arithmetic capabilities in large language models (LLMs).</p> <h4 id="arithmetic-length-generalization-dataset-algd">Arithmetic Length Generalization Dataset (ALGD)</h4> <p>To evaluate our model’s performance on elementary arithmetic operations, we designed a custom dataset with 3,500 examples for addition, multiplication, subtraction, division, and modulus. Each operation includes 100 examples with 1-digit to 7-digit numbers (~around 700 per operation), allowing us to assess the model’s capacity to handle progressively more complex scenarios.</p> <h3 id="evaluation-methodology">Evaluation methodology</h3> <p>We assess the model’s arithmetic reasoning using accuracy metric on the GSM-8K and ALGD datasets. For GSM-8K, accuracy measures the percentage of correctly answered questions, benchmarking general mathematical reasoning and comparing with prior work. On ALGD, we measure digit-wise accuracy separately across digit complexities for all operations, evaluating the model’s scalability and robustness with numerical complexity along with overall accuracy. This comprehensive approach highlights the model’s strengths and limitations in diverse mathematical tasks.</p> <h3 id="experiment-details">Experiment details</h3> <p>We use the configurations defined in <a href="#tab-model-config">Table 1</a> to evaluate GPT-4o-mini, Llama-3.1-70b-Instruct, Llama-3.1-8b-Instruct, and gemma-2b-it on both GSM-8K and ALGD datasets. We explicitly specify in the system prompt to not use any code to avoid GPT-4o-mini from using its Code Interpreter feature. The exact prompt for different models is shown below. We first compare the performance of both models on GSM-8K to establish a baseline. Inspired by GSM-Symbolic <d-cite key="Mirzadeh2024"></d-cite>, we then increased the number of digits to increase the complexity of the questions and to move away from the training distribution. Finally, we evaluate the models on our generated ALGD dataset. We use Tuned Lens <d-cite key="belrose2023eliciting"></d-cite>, built upon the Logit Lens <d-cite key="nostalgebraist_2022"></d-cite> approach, to project the activations from intermediate layers of Llama-3.1-8B-Instruct into the model’s vocabulary space. Tuned Lens helps avoid misalignment observed in Logit Lens due to evolution of representations by learning layer-specific probes to minimize the distribution shift.</p> <div id="tab-model-config"> <strong>Table 1:</strong> Model configuration parameters used in experiments. </div> <table> <thead> <tr> <th>Parameter</th> <th>Description</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">max_completion_tokens</code></td> <td>Max # of completion tks including output and reasoning tks</td> <td>512</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">temperature</code></td> <td>Sampling temperature</td> <td>0.7</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">top_p</code></td> <td>Probability mass to sample tokens from</td> <td>0.9</td> </tr> <tr> <td>$n$</td> <td># of sequences generated</td> <td>1</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">seed</code></td> <td>Seed for random number generator</td> <td>0</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GPT-4o-mini prompt
</span><span class="n">messages</span> <span class="o">=</span><span class="p">[</span>
  <span class="p">{</span>
    <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant. Do not use or generate code in your responses. Please respond with only the final answer, formatted as a single number without additional explanation or context.</span><span class="sh">"</span><span class="p">},</span>
  <span class="p">{</span>
    <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s"> </span><span class="se">\n\n</span><span class="s">Please provide only the final answer as a single number.</span><span class="sh">"</span>
  <span class="p">}</span>
<span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Prompt for Llama and Gemma models
</span><span class="n">messages</span> <span class="o">=</span><span class="p">[</span>
  <span class="p">{</span>
    <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span> <span class="p">,</span>
    <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">You are a helpful assistant. Do not use or generate code in your responses . Also append the final numerical answer on a new line append with </span><span class="sh">'</span><span class="s">#### </span><span class="sh">'"</span> <span class="p">}</span> <span class="p">,</span>
  <span class="p">{</span>
    <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span>
  <span class="p">}</span>
<span class="p">]</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>It can be seen from <a href="#gsm-8k-eval">Fig. 1</a> that frontier models perform with $\sim$94% accuracy on the GSM-8K dataset. However, when the number of digits in the numerical values are increased, our observations reveal that models perform well with numbers containing fewer unique digits, such as 480000, but their accuracy declines with numbers that have more unique digits, even on a small scale (e.g., 48346) (Refer Appendix 6.2.3 in the report). Additionally, the models often misrepresent high-magnitude numbers, sometimes interpreting quadrillions as billions or using inconsistent scientific notation. As numeric complexity increases, models occasionally “hallucinate” numbers, particularly in responses to simple arithmetic prompts involving large sums. When using Few-Shot prompting, models show an over-fitting tendency to the set number of reasoning steps, producing accurate intermediate calculations but failing to combine these results correctly in the final answer. With Chain-of-Thought prompting, while models can decompose the operation between large numbers into those with smaller numbers and generate accurate results, they struggle with summing these intermediate results accurately.</p> <figure id="gsm-8k-eval"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/GSM-8K%20Performance.png" alt="Performance of SoTA models on GSM-8K"/> <figcaption> Figure 1: Frontier models like GPT-4o-mini and Llama-3.1-70b-Instruct score &gt;90% on mathematical reasoning datasets like GSM-8K. *Due to the constraint max_completion_tokens set to 512, 6 out of 1319 responses by GPT-4o-mini were truncated. Accuracy is calculated after removing these question-answer pairs. </figcaption> </figure> <figure id="fig-addition"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/addition-length-generalization.png" alt="Addition Length Generalization Performance"/> <figcaption>Figure 2a: Performance deteriorates with increase in number of digits. Llama-3.1-70b-Instruct performs better than GPT-4o-mini whose performance drops after 5-digit addition.</figcaption> </figure> <figure id="fig-multiplication"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/multiplication-length-generalization-performance.png" alt="Multiplication Length Generalization Performance"/> <figcaption>Figure 2b: Performance of both Llama-3.1-70b-Instruct and GPT-4o-mini reaches 0% after 3-digit multiplication.</figcaption> </figure> <p>We observe performance of addition <a href="#fig-addition">Fig. 2a</a> and multiplication <a href="#fig-multiplication">Fig. 2b</a> limiting to 6-digit and 3-digit numbers, respectively, but exhibit significant errors beyond these limits. Furthermore, our results show that while models can generally handle computations involving numbers up to 6 digits, they often fail for higher orders, with larger unique digits in smaller numbers exacerbating error rates. One interesting observation is that, the models often generate the first and last few digits accurately, even as overall accuracy declines (<a href="#digit-match-gpt-mul">Fig. 3a</a>, <a href="#digit-match-llama-mul">Fig. 3b</a>) . This could be related to frequencies of combination of specific digits in the training corpus or specific tokenization methods. More graphs can be seen in Appendix sections 6.3 and 6.4 in the report. We plan to explore further in this direction. The trend remains the same irrespective of different training recipes and tokenization strategies across the two models.</p> <figure> <div> <div id="digit-match-gpt-mul"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_gpt_mul.png" alt="GPT-4o-mini 7-digit matches"/> <figcaption>Figure 3a: GPT-4o-mini digit accuracy.</figcaption> </div> <div id="digit-match-llama-mul"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_llama-70b_mul.png" alt="Llama-3.1-70b 7-digit matches"/> <figcaption>Figure 3b: Llama-3.1-70b digit accuracy.</figcaption> </div> </div> <br/> <figcaption>Figure 3: Digit accuracy for multiplication. The trend remains the same irrespective of different training recipes and tokenization strategies across the two models.</figcaption> </figure> <p>With tuned lens, it can be seen that the final numerical answer is generated in the later layers of Llama-3.1-8B <a href="#3-digit-tuned_lens_success">Fig 4a</a>. In case of multi-token answers, even after providing the first token in the prompt, the model fails to generate the next token <a href="#3-digit-tuned_lens_fail">Fig 4b</a>.</p> <figure> <div> <div id="3-digit-tuned_lens_success"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/348%20+%20456.png" alt="GPT-4o-mini 7-digit matches"/> <figcaption>Figure 4a: Correct Response: TunedLens for Llama-3.1-8b-Instruct for 3-digit addition.</figcaption> </div> <div id="3-digit-tuned_lens_fail"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/466%20+%20488.png" alt="Llama-3.1-70b 7-digit matches"/> <figcaption>Figure 4b: Incorrect Response: TunedLens for Llama-3.1-8b-Instruct for 3-digit addition.</figcaption> </div> </div> <br/> <figcaption>Figure 4: Tuned Lens Plots for Llama-3.1-8b-Instruct (Cross Entropy Loss).</figcaption> </figure> <h3 id="more-multi-digit-multiplication-accuracy-graphs">More Multi-digit Multiplication Accuracy Graphs</h3> <figure id="fig-mul-acc-llama3.1-8b"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_llama_8b_mul.png" alt="Multiplication Length Generalization Performance"/> <figcaption>Multiplication Performance of Llama-3.1-8b-Instruct</figcaption> </figure> <figure id="fig-mul-acc-gemma2-2b"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_gemma_2b_mul.png" alt="Multiplication Length Generalization Performance"/> <figcaption>Multiplication Performance of Gemma-2-2b-Instruct</figcaption> </figure> <h3 id="multi-digit-addition-accuracy-graphs">Multi-digit Addition Accuracy Graphs</h3> <figure id="fig-add-acc-gpt-4o-mini"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_gpt_add.png" alt="Addition Length Generalization Performance"/> <figcaption>Addition Performance of GPT-4o-mini</figcaption> </figure> <figure id="fig-add-acc-llama3.1-70b"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_add_llama-70b.png" alt="Addition Length Generalization Performance"/> <figcaption>Addition Performance of Llama-3.1-70b-Instruct</figcaption> </figure> <figure id="fig-add-acc-llama3.1-8b"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_llama-8-add.png" alt="Addition Length Generalization Performance"/> <figcaption>Addition Performance of Llama-3.1-8b-Instruct</figcaption> </figure> <figure id="fig-add-acc-gemma2-2b"> <img src="/assets/img/2025-02-03-characterizing-arithmetic-length-generalization/digit_match_accuracy_gemma_add.png" alt="Addition Length Generalization Performance"/> <figcaption>Addition Performance of Gemma-2-2b-Instruct</figcaption> </figure> <h2 id="conclusion">Conclusion</h2> <p>This research provides valuable insights into the current capabilities and limitations of LLMs in mathematical reasoning. While models like GPT-4o-mini and LLaMa-3.1-70B demonstrate impressive performance on standard benchmarks, they struggle with unbounded multistep computations involving complex numbers.</p> <p>Understanding these limitations is crucial as we continue developing and deploying these models in real-world applications where mathematical reasoning may be required. Our findings contribute to the broader conversation about how to enhance and complement LLM capabilities in domains requiring precise numerical computation.</p> <p>For those interested in exploring this further, our dataset and code are publicly available at our GitHub repository: <a href="https://github.com/raishish/arithmetic-interp">https://github.com/raishish/arithmetic-interp</a>.</p> <h2 id="citation">Citation</h2> <p>If you would like to cite this work, please use the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rai2024arithmetic-perf-length-generalization</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{How do large language models perform arithmetic operations?}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Rai, Ashish and Peddaputha, Akash and Gupta, Aman}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{Dec}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://raishish.github.io/2024/12/02/llm-arithmetic-interp.html}</span>
<span class="p">}</span>
</code></pre></div></div> ]]></content><author><name>Ashish Rai</name></author><category term="llms"/><category term="autoregressive"/><category term="length-generalization"/><summary type="html"><![CDATA[An initial exploration of a mechanistic understanding of arithmetic performance (and performance scaling) in large language models.]]></summary></entry></feed>