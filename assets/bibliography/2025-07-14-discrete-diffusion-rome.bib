@article{berglund2023reversal,
  title={The Reversal Curse: LLMs Trained on "A is B" Fail to Learn "B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023},
  url={https://arxiv.org/abs/2309.12288},
}

@inproceedings{kitouni2024factorization,
  title={The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More},
  author={Kitouni, Ouail and Elhage, Nelson and Brown, James and Michael, John and Anthropic Mechanistic Interpretability Team and Kaplan, Jared},
  booktitle={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}

@article{ma2023untying,
  title={Untying the Reversal Curse via Bidirectional Language Model Editing},
  author={Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan and Liu, Cong},
  journal={arXiv preprint arXiv:2310.10322},
  year={2023}
}

@article{nie2024scaling,
  title={Scaling up Masked Diffusion Models on Text},
  author={Nie, Shen and others},
  journal={arXiv preprint arXiv:2410.18514},
  year={2024}
}

@article{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and others},
  journal={arXiv preprint arXiv:2406.07524},
  year={2024}
}

@article{wu2023exploring,
  title={Exploring the Reversal Curse and Other Deductive Logical Reasoning in BERT and GPT-Based Large Language Models},
  author={Wu, Da and Wang, Lingfei},
  journal={arXiv preprint arXiv:2312.03633},
  year={2023}
}

@misc{nie2025largelanguagediffusionmodels,
      title={Large Language Diffusion Models},
      author={Shen Nie and Fengqi Zhu and Zebin You and Xiaolu Zhang and Jingyang Ou and Jun Hu and Jun Zhou and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
      year={2025},
      eprint={2502.09992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.09992},
}

@misc{warner2024smarterbetterfasterlonger,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663},
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683},
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{Meng2022LocatingAE,
      title={Locating and Editing Factual Associations in GPT},
      author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
      year={2022},
      eprint={2202.05262},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.05262},
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@inproceedings{10.5555/2074022.2074073,
author = {Pearl, Judea},
title = {Direct and indirect effects},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {411–420},
numpages = {10},
location = {Seattle, Washington},
series = {UAI'01}
}

@article{kim2025train,
  title        = {Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions},
  author       = {Jaeyeon Kim and Kulin Shah and Vasilis Kontonis and Sham Kakade and Sitan Chen},
  journal      = {arXiv preprint arXiv:2502.06768},
  year         = {2025},
  archivePrefix= {arXiv},
  eprint       = {2502.06768},
  primaryClass = {cs.LG},
  note         = {License: CC BY 4.0},
  url          = {https://arxiv.org/abs/2502.06768}
}

@article{corvelo2025evaluation,
  title        = {Evaluation of Large Language Models via Coupled Token Generation},
  author       = {Nina Corvelo Benz and Stratis Tsirtsis and Eleni Straitouri and Ivi Chatzi and Ander Artola Velasco and Suhas Thejaswi and Manuel Gomez-Rodriguez},
  journal      = {arXiv preprint arXiv:2502.01754},
  year         = {2025},
  archivePrefix= {arXiv},
  eprint       = {2502.01754},
  primaryClass = {cs.CL},
  note         = {License: CC BY 4.0},
  url          = {https://arxiv.org/abs/2502.01754}
}

@article{wang2021kepler,
  title   = {KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
  author  = {Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhengyan Zhang and Zhiyuan Liu and Juanzi Li and Jian Tang},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {9},
  pages   = {176--194},
  year    = {2021},
  url     = {https://aclanthology.org/2021.tacl-1.11},
  publisher = {MIT Press}
}