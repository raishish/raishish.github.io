@article{Mirzadeh2024,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{Zhang2024,
  title={Interpreting and Improving Large Language Models in Arithmetic Calculation},
  author={Zhang, Wei and Wan, Chaoqun and Zhang, Yonggang and Cheung, Yiu-ming and Tian, Xinmei and Shen, Xu and Ye, Jieping},
  journal={Proceedings of the 41st International Conference on Machine Learning},
  year={2024}
}

@article{Zhou2024,
  title={Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia},
  author={Zhou, Zhejian and Wang, Jiayu and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2409.17391},
  year={2024}
}

@article{Ahn2024,
  title={Large Language Models for Mathematical Reasoning: Progresses and Challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}

@article{Zhou2024_Algorithms,
  title={What Algorithms Can Transformers Learn? A Study in Length Generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={Proceedings of ICLR 2024},
  year={2024}
}

@article{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems},
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{stolfo2023understanding,
  title={Understanding arithmetic task performance in pre-trained LLMs: Insights and limitations},
  author={Stolfo, Saadia and K, Vishak and Livschitz, Leonid},
  journal={Proceedings of the ACL Conference on Computational Linguistics},
  year={2023}
}

@article{wu2023causal,
  title={Causal abstraction in large language models for arithmetic operations},
  author={Wu, Anthony and Wang, David and Hall, Jonathan},
  journal={arXiv preprint arXiv:2309.04200},
  year={2023}
}

@article{radford2019gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI GPT-2 Technical Report},
  year={2019}
}

@inproceedings{panigrahi2023efficient,
  title={Efficient mathematical reasoning in large language models using task-specific parameter fine-tuning},
  author={Panigrahi, Shalini and Uesato, Tomoya and Gal, Yarin},
  booktitle={Proceedings of the 38th International Conference on Machine Learning},
  year={2023}
}

@article{imani2023interpreting,
  title={Interpreting and improving large language models in arithmetic calculation: Insights from path patching},
  author={Imani, Hadi and Liang, Kevin and Nguyen, Emma},
  journal={arXiv preprint arXiv:2409.01659v1},
  year={2023}
}

@inproceedings{uesato2022reinforcement,
  title={Reinforcement learning for mathematical reasoning in large language models},
  author={Uesato, Tomoya and Czarnecki, Wojciech Marian and Kohli, Pushmeet},
  booktitle={Proceedings of NeurIPS},
  year={2022}
}



@misc{jiang2024peektokenbiaslarge,
      title={A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners},
      author={Bowen Jiang and Yangxinyu Xie and Zhuoqun Hao and Xiaomeng Wang and Tanwi Mallick and Weijie J. Su and Camillo J. Taylor and Dan Roth},
      year={2024},
      eprint={2406.11050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11050},
}


@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774},
}

@misc{gunter2024appleintelligencefoundationlanguage,
      title={Apple Intelligence Foundation Language Models},
      author={Apple Intelligence Team},
      year={2024},
      eprint={2407.21075},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21075},
}

@misc{lee2023teachingarithmeticsmalltransformers,
      title={Teaching Arithmetic to Small Transformers},
      author={Nayoung Lee and Kartik Sreenivasan and Jason D. Lee and Kangwook Lee and Dimitris Papailiopoulos},
      year={2023},
      eprint={2307.03381},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.03381},
}
@misc{dziri2023faithfatelimitstransformers,
      title={Faith and Fate: Limits of Transformers on Compositionality},
      author={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jiang and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},
      year={2023},
      eprint={2305.18654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.18654},
}

@misc{opedal2024languagemodelsexhibitcognitive,
      title={Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?},
      author={Andreas Opedal and Alessandro Stolfo and Haruki Shirakami and Ying Jiao and Ryan Cotterell and Bernhard Sch√∂lkopf and Abulhair Saparov and Mrinmaya Sachan},
      year={2024},
      eprint={2401.18070},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.18070},
}

@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}

@misc{nostalgebraist_2022,
  author = {nostalgebraist},
  title = {Logit Lens},
  url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
  year = {2022},
  howpublished = {LessWrong},
  note = {Blog post}
}

@article{belrose2023eliciting,
  title={Eliciting latent predictions from transformers with the tuned lens},
  author={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2303.08112},
  year={2023}
}